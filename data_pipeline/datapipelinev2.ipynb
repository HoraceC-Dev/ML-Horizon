{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langgraph.graph import END, StateGraph\n",
    "# For State Graph \n",
    "from typing_extensions import TypedDict\n",
    "import os\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import Tool\n",
    "from typing import Optional\n",
    "from langchain_core.tools import StructuredTool\n",
    "import json\n",
    "from langchain.output_parsers.structured import (\n",
    "StructuredOutputParser, ResponseSchema\n",
    ")\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers.structured import (\n",
    "StructuredOutputParser, ResponseSchema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "AWS_ACCESS_KEY= os.getenv(\"AWS_ACCESS_KEY\")\n",
    "llm = ChatBedrock(\n",
    "    aws_access_key_id=AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    model_id=\"anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "    model_kwargs=dict(temperature=0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content=\"GPUs (Graphics Processing Units) are better suited for AI and machine learning tasks compared to CPUs (Central Processing Units) for several key reasons:\\n\\n1. Parallel Processing:\\n- GPUs have thousands of smaller cores designed for parallel processing\\n- AI/ML tasks often involve many simultaneous calculations that can be performed in parallel\\n- CPUs have fewer cores (typically 4-16) optimized for sequential processing\\n\\n2. Matrix Operations:\\n- AI/ML involves heavy matrix multiplication and vector operations\\n- GPUs are specifically designed to handle these types of calculations efficiently\\n- Modern GPUs include specialized tensor cores for deep learning operations\\n\\n3. Memory Bandwidth:\\n- GPUs have higher memory bandwidth\\n- Can transfer large amounts of data quickly\\n- Important for handling large datasets and model parameters\\n\\n4. Specialized Architecture:\\n- Built for handling repetitive, simple calculations\\n- Optimized for floating-point operations\\n- Better suited for the mathematical operations common in AI/ML\\n\\n5. Cost-Effectiveness:\\n- Better performance per dollar for AI/ML workloads\\n- More energy-efficient for parallel processing tasks\\n- Higher throughput for training neural networks\\n\\n6. Deep Learning Libraries:\\n- Major AI frameworks (TensorFlow, PyTorch) are optimized for GPU processing\\n- CUDA (NVIDIA's parallel computing platform) enables efficient GPU utilization\\n\\nThese advantages make GPUs significantly faster than CPUs for most AI and machine learning applications, especially in training deep neural networks.\" additional_kwargs={'usage': {'prompt_tokens': 21, 'completion_tokens': 319, 'total_tokens': 340}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-5-sonnet-20241022-v2:0'} response_metadata={'usage': {'prompt_tokens': 21, 'completion_tokens': 319, 'total_tokens': 340}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-5-sonnet-20241022-v2:0'} id='run-0483b978-7011-469e-9f2c-11aea1f65ac4-0' usage_metadata={'input_tokens': 21, 'output_tokens': 319, 'total_tokens': 340}\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"Why is gpu better than cpu in ai and mahcine learning?\")\n",
    "print(type(result))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def question_extractor_chain(state):\n",
    "    print(\"starting now\\n\\n\\n\")\n",
    "\n",
    "    question_explorer_prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Role: Search Engine Question Optimizer Robot\n",
    "\n",
    "Scenario:\n",
    "User has a question that he/she wants to know from the internet not from you.\n",
    "\n",
    "Task: Base on user question, try to dive deeper to the intent, the technical aspects of the query, and optimize user's question for search engine.\n",
    "You are not responsible in answering user's question. You need to generate 10 search query based on the user's question\n",
    "that allows user to obtain the best informtaion from the internet through the search engine, NOT FROM YOU. \n",
    "The generated questions should not enquire user to input information, instead, they should be ready for search engine. \n",
    "Optimize all the questions for search engine that enables user to obtain high quality information. \n",
    "\n",
    "This is the user's question: {question}\n",
    "\n",
    "Instructions:\n",
    "Think step by step to generate 10 search engine queries at the best of your ability.\n",
    "Assume user will copy and paste directly without any changes, so there shouldn't be [] variable that allows user to change.\n",
    "\n",
    "Output Format: No greeting, no bold text, no Italic text, just plain text in string, no number index\n",
    "\"\"\"\n",
    "    ).partial(time=datetime.now())\n",
    "\n",
    "\n",
    "    question_explorer_chain = question_explorer_prompt | llm\n",
    "    if \"list_of_questions\" in state:\n",
    "        temp_list = state[\"list_of_questions\"]\n",
    "    else:\n",
    "        temp_list = []\n",
    "\n",
    "    for question in state[\"org_questions\"]:\n",
    "        result = question_explorer_chain.invoke(question)\n",
    "        temp_list.extend(result[\"list_of_queries\"])\n",
    "\n",
    "    return {\"list_of_questions\": temp_list}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
